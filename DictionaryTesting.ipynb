{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a5eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/helmadevina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/helmadevina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/helmadevina/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "main() missing 1 required positional argument: 'openai_api_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 234\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# === Run\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 234\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdictionary_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdictionary.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43minput_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhost_descriptions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43minput_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhost_about\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43moutput_clean_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcleaned_host_descriptions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43moutput_dict_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_theme_dictionary.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43moutput_count_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtheme_counts_per_host.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;66;43;03m#openai_api_key=\"\",  # replace with your key\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43muse_synonyms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-ada-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Processing completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Base dictionary themes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: main() missing 1 required positional argument: 'openai_api_key'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# === Step 0: Load the Base Dictionary from CSV ===\n",
    "def load_dictionary_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    theme_dict = {}\n",
    "    for theme in df['theme'].unique():\n",
    "        theme_dict[theme] = df[df['theme'] == theme]['item'].dropna().tolist()\n",
    "    return theme_dict\n",
    "\n",
    "# === Step 1: Cleaning & Lemmatizing (Single Words Only) ===\n",
    "def clean_and_lemmatize_single_words(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Filter out very short words (optional)\n",
    "    lemmatized = [word for word in lemmatized if len(word) > 1]\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "def clean_and_lemmatize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# === Step 2: Synonym Grouping ===\n",
    "def find_synonyms(word):\n",
    "    if ' ' in word:\n",
    "        return []\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            syn_name = lemma.name().replace('_', ' ')\n",
    "            if syn_name != word:\n",
    "                synonyms.add(syn_name)\n",
    "    return list(synonyms)\n",
    "\n",
    "def group_synonyms(word_list):\n",
    "    synonym_groups = {}\n",
    "    processed_words = set()\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word in processed_words:\n",
    "            continue\n",
    "        if ' ' not in word:\n",
    "            syns = find_synonyms(word)\n",
    "            group = [word]\n",
    "            for syn in syns:\n",
    "                if syn in word_list and syn != word:\n",
    "                    group.append(syn)\n",
    "                    processed_words.add(syn)\n",
    "            synonym_groups[word] = group\n",
    "        else:\n",
    "            synonym_groups[word] = [word]\n",
    "        processed_words.add(word)\n",
    "    \n",
    "    return synonym_groups\n",
    "\n",
    "# === Step 3: Embedding Functions ===\n",
    "def get_embedding(text, model_name=\"text-embedding-ada-002\"):\n",
    "    try:\n",
    "        response = openai.Embedding.create(input=[text], model=model_name)\n",
    "        return response['data'][0]['embedding']\n",
    "    except Exception as e:\n",
    "        if \"Rate limit\" in str(e):\n",
    "            print(f\"Rate limit hit, waiting 1 second before retrying...\")\n",
    "            time.sleep(1)  # Wait 1 second before retrying\n",
    "            try:\n",
    "                response = openai.Embedding.create(input=[text], model=model_name)\n",
    "                return response['data'][0]['embedding']\n",
    "            except Exception as retry_e:\n",
    "                print(f\"Error embedding '{text}' after retry: {retry_e}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Error embedding '{text}': {e}\")\n",
    "            return None\n",
    "\n",
    "# === Step 4: Find Unique Words/Phrases ===\n",
    "def extract_unique_items(all_items, existing_dict):\n",
    "    existing_items = set()\n",
    "    for theme_items in existing_dict.values():\n",
    "        existing_items.update(theme_items)\n",
    "    return list(set(all_items) - existing_items)\n",
    "\n",
    "# === Step 5: Assign New Words to Themes ===\n",
    "def assign_to_theme(new_items, theme_dict, threshold=0.8, model_name=\"text-embedding-ada-002\"):\n",
    "    theme_embeddings = {}\n",
    "\n",
    "    for theme, items in theme_dict.items():\n",
    "        item_embeddings = []\n",
    "        for item in items:\n",
    "            emb = get_embedding(item, model_name)\n",
    "            if emb is not None:\n",
    "                item_embeddings.append(emb)\n",
    "        if item_embeddings:\n",
    "            theme_embeddings[theme] = np.mean(item_embeddings, axis=0)\n",
    "        else:\n",
    "            print(f\"No valid embeddings for theme {theme}\")\n",
    "\n",
    "    assignments = {}\n",
    "    for item in tqdm(new_items, desc=\"Assigning new words/phrases\"):\n",
    "        item_emb = get_embedding(item, model_name)\n",
    "        if item_emb is None:\n",
    "            continue\n",
    "        \n",
    "        sims = {}\n",
    "        for theme, emb in theme_embeddings.items():\n",
    "            sims[theme] = cosine_similarity(\n",
    "                np.array(item_emb).reshape(1, -1),\n",
    "                np.array(emb).reshape(1, -1)\n",
    "            )[0][0]\n",
    "        \n",
    "        best_theme = max(sims, key=sims.get)\n",
    "        if sims[best_theme] >= threshold:\n",
    "            assignments[item] = best_theme\n",
    "\n",
    "    return assignments\n",
    "\n",
    "# === Step 6: Update Dictionary ===\n",
    "def update_dictionary(original_dict, new_assignments):\n",
    "    updated_dict = original_dict.copy()\n",
    "    for item, theme in new_assignments.items():\n",
    "        updated_dict[theme].append(item)\n",
    "    return updated_dict\n",
    "\n",
    "# === Step 7: Word Counting Per Theme with Original Word Count ===\n",
    "def count_items_per_theme(texts, original_descriptions, theme_dict):\n",
    "    results = []\n",
    "    for idx, (text_items, orig_desc) in enumerate(zip(texts, original_descriptions)):\n",
    "        # Get original word count\n",
    "        orig_tokens = word_tokenize(orig_desc)\n",
    "        orig_word_count = len(orig_tokens)\n",
    "        \n",
    "        # Count themed word occurrences\n",
    "        theme_counts = {theme: 0 for theme in theme_dict}\n",
    "        for item in text_items:\n",
    "            for theme, theme_items in theme_dict.items():\n",
    "                if item in theme_items:\n",
    "                    theme_counts[theme] += 1\n",
    "                    break\n",
    "        \n",
    "        row = {\n",
    "            \"host_id\": idx, \n",
    "            \"total_items\": len(text_items),\n",
    "            \"original_word_count\": orig_word_count\n",
    "        }\n",
    "        \n",
    "        for theme in theme_dict:\n",
    "            row[f\"count_{theme}\"] = theme_counts[theme]\n",
    "            # Calculate proportions based on original word count\n",
    "            row[f\"prop_{theme}\"] = theme_counts[theme] / orig_word_count if orig_word_count > 0 else 0\n",
    "        \n",
    "        results.append(row)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === Step 8: Main ===\n",
    "def main(dictionary_csv_path, input_csv, input_column, output_clean_csv, output_dict_csv, output_count_csv,\n",
    "         openai_api_key, use_synonyms=True, embedding_model=\"text-embedding-ada-002\"):\n",
    "    \n",
    "    openai.api_key = openai_api_key\n",
    "\n",
    "    base_dict = load_dictionary_from_csv(dictionary_csv_path)\n",
    "\n",
    "    df = pd.read_csv(input_csv)\n",
    "    descriptions = df[input_column].dropna().tolist()\n",
    "\n",
    "    # Use single word approach\n",
    "    processed_texts = [clean_and_lemmatize_single_words(desc) for desc in descriptions]\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"host_id\": range(len(processed_texts)),\n",
    "        \"processed_text\": [' '.join(items) for items in processed_texts]\n",
    "    }).to_csv(output_clean_csv, index=False)\n",
    "\n",
    "    all_unique_items = list(set([item for text in processed_texts for item in text]))\n",
    "\n",
    "    if use_synonyms:\n",
    "        synonym_groups = group_synonyms(all_unique_items)\n",
    "        synonym_map = {}\n",
    "        for rep_word, synonyms in synonym_groups.items():\n",
    "            for syn in synonyms:\n",
    "                synonym_map[syn] = rep_word\n",
    "        for i, text in enumerate(processed_texts):\n",
    "            processed_texts[i] = [synonym_map.get(item, item) for item in text]\n",
    "        all_unique_items = list(set([item for text in processed_texts for item in text]))\n",
    "\n",
    "    new_items = extract_unique_items(all_unique_items, base_dict)\n",
    "    new_assignments = assign_to_theme(new_items, base_dict, model_name=embedding_model)\n",
    "    final_dict = update_dictionary(base_dict, new_assignments)\n",
    "\n",
    "    dict_rows = []\n",
    "    for theme, items in final_dict.items():\n",
    "        for item in items:\n",
    "            dict_rows.append({\"theme\": theme, \"item\": item})\n",
    "    pd.DataFrame(dict_rows).to_csv(output_dict_csv, index=False)\n",
    "\n",
    "    # Use original descriptions for proportions\n",
    "    count_df = count_items_per_theme(processed_texts, descriptions, final_dict)\n",
    "    count_df.to_csv(output_count_csv, index=False)\n",
    "\n",
    "    return {\n",
    "        \"base_dict\": base_dict,\n",
    "        \"final_dict\": final_dict,\n",
    "        \"new_assignments\": new_assignments,\n",
    "        \"counts\": count_df\n",
    "    }\n",
    "\n",
    "# === Run\n",
    "if __name__ == \"__main__\":\n",
    "    result = main(\n",
    "         dictionary_csv_path=\"dictionary.csv\",\n",
    "         input_csv=\"host_descriptions.csv\",\n",
    "         input_column=\"host_about\",\n",
    "         output_clean_csv=\"cleaned_host_descriptions.csv\",\n",
    "         output_dict_csv=\"final_theme_dictionary.csv\",\n",
    "         output_count_csv=\"theme_counts_per_host.csv\",\n",
    "         #openai_api_key=\"\",  # replace with your key\n",
    "         use_synonyms=True,\n",
    "         embedding_model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "\n",
    "    print(\"✅ Processing completed!\")\n",
    "    print(f\"✅ Base dictionary themes: {list(result['base_dict'].keys())}\")\n",
    "    print(f\"✅ New assignments: {len(result['new_assignments'])}\")\n",
    "    print(f\"✅ Final counts shape: {result['counts'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b296ca-0331-417e-acd5-9ba99d3879d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['theme', 'item'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dictionary.csv')\n",
    "print(df.columns)  # See what columns actually exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c49ed03-3380-4217-894d-67e79b03ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"showing_personal_charisma\": [],\n",
      "  \"presenting_friendly_attitudes\": [],\n",
      "  \"promoting_socialisation_and_sharing\": [],\n",
      "  \"offering_service_and_assistance\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "docx_dictionary_path=\"dictionary-raw.docx\"\n",
    "base_dict = parse_word_file(docx_dictionary_path)\n",
    "print(json.dumps(base_dict, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e974077-b8e7-449c-bea6-bd0ce2d969f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
